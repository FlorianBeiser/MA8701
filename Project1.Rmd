---
title: "Data Analysis Project 1"
author: 'Group 5 : Yellow Submarine'
date: "`r format(Sys.time(), '15 February, 2021')`"
output:
  pdf_document: default
  html_document: default
subtitle: MA8701
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
```

In this project, we analyse a real dataset using shrinkage methods from part 1 of the MA8701 course.

### Note on Open Science

To pursue the idea of reproducible research, the chosen dataset as well as the code for our analysis are publicly accessible:

* dataset: https://data.ub.uni-muenchen.de/2/1/miete03.asc
* code: https://github.com/FlorianBeiser/MA8701

```{r rpackages, eval=TRUE, include=FALSE}
#load packages
library(GGally)
library(tidyverse)
library(corrplot)
```


# The Data Set 

For our project work we use the Munich Rent 2003 data set as described in https://rdrr.io/cran/LinRegInteractive/man/munichrent03.html. The data set has 12 original covariates, where a brief introduction to these parameters is listed below (in brackets the type of the covariate is explicated), and 2053 observations are available:

-   `nmqm`: rent per square meter (double)
-   `wfl`: area in square meters (int)
-   `rooms`: number of rooms (int)
-   `bj`: year of construction (Factor)
-   `bez`: district (Factor)
-   `wohngut`: quality of location (int)
-   `wohnbest`: high quality of location (int)
-   `ww0`: hot water supply available (int)
-   `zh0`: central heating (int)
-   `badkach0`: tiled bathroom (int)
-   `badextra`: high-quality bathroom (int)
-   `kueche`: upscale kitchen equipment (int)

and the response

-   `nm`: rental price (double).

The label "double" naturally stands for numerical values, "int" categorizes parameters with integer values, and "Factor" symbolizes parameter taking a certain number of levels - where in contrast to integers a higher level does not necessarily mean an improvement. 

```{r loadData, eval=TRUE, include=FALSE}
# Load data set
munich_house <- read.table(
  "https://data.ub.uni-muenchen.de/2/1/miete03.asc",
  sep="\t", header=TRUE)

print(munich_house)
```

Since the price per square meter `nmqm` multiplied with the area `wfl` directly gives the rental price `nm` which we define as the response in the system, it does not make sense to keep both values. Hence, we exclude `nmqm` from the data set to avoid it consuming all the significance in the coming data analysis. 


```{r dataPlot, echo=FALSE, fig.cap="\\label{fig:data}Exploration of multi-colinearity in the data set"} 
# Correlation plot with excluding nmqm, so to find the colinearlity
# Observe correlations between covariates
xs=model.matrix(nm~.-bj -bez,data=munich_house)[,-c(1:2)] # to take care of categorical variables, but not include the intercept column
xss=scale(xs)
corrplot.mixed(cor(xss),upper = "ellipse", lower = "number", tl.cex = 0.5)
```

Figure \ref{fig:data} shows the correlation between covariates ignoring the factorials and reveals that the dataset may suffer from a very light multi-colinearity. 

However, the factorial variable `bj` and `bez` introduce 44 and 25 levels, respectively, leading to relatively unclear dependencies between the full covariate set (including factorial variables) and the response, which makes this dataset suitable for a regression analysis and the application of shrinkage methods. 

```{r buildDataFrame, eval=TRUE, include=FALSE} 
# 
munich_house$bez=as.factor(munich_house$bez)
munich_house$bj=as.factor(munich_house$bj)
str(munich_house)

#create model matrix without intercept and without nmqm
x_mod <- model.matrix(nm~.-nmqm,data=munich_house)[,-1]
y_mod <- munich_house$nm
df_mod <- data.frame(y_mod,x_mod)
colnames(df_mod)[1]="nm"
```


# Data Analysis

Subsequently, we start with a plain linear regression model as reference such that we can particularly point out the benefits of shrinkage approaches. As shrinkage methods, we employ the ridge, the lasso and the group lasso. The latter approach seems to be very well suited for our data set, as it allows to take the factorial variables as a single unit for shrinkage. 


## Regression

We start the data analysis with a vanilla LM regression for reference using R's internal `lm` functionality

```{r regression, eval=TRUE, include=FALSE}
# Vanilla regression model
lm_mod <- lm(nm~.,data=df_mod) 
#Printing the summary to assess significant parameters
summary(lm_mod)
```

The the regression results show a lot of significant covariates. As maybe expected, the area `wfl` is strongly related to the rent price, however confusingly, the significance of different levels of the years of construction `bj` and districts `bez` varies a lot. From those both observations, it is not possible, to extract clear data analysis results, which also would match our interpretation of the problem. An extract of the results can be found later, when we summarize the outcomes of all methods.


## Ridge Shrinkage

As first shrinkage method, we consider the ridge regression that uses Tikhonov regularisation in the model, where we utilize the `glmnet` library for its implementation in R. Since ridge introduces the additional tuning parameter $\lambda$ we perform cross validation for the model selection, i.e. for the choice of the optimal $\lambda$, where we follow the advice in the ELS to choose $\lambda$ as the one with minimal CV-error plus one standard deviation of the CV-error. 

```{r ridge, eval=TRUE, include=FALSE}
# Import glmnet library for ridge
library(glmnet)

# Massaged lambda grid
gen_mod_for_lambda <- glmnet(x = x_mod, y = y_mod, standardize=TRUE, alpha = 0)
my_lambda <- gen_mod_for_lambda$lambda
my_lambda <- c(my_lambda, 10, 5, 3, 1, 0.5, 0.1) # add more to approach zero lambda

# Model definition
ridge_fit <- glmnet(x_mod, y_mod, standardize=TRUE, alpha = 0, lambda = my_lambda)

#Choosing optimal lambda performing Crossvalidation
cv_ridge <- cv.glmnet(x_mod, y_mod, standardize=TRUE, alpha = 0, lambda = my_lambda)

print(paste("The lamda giving the smallest CV error",cv_ridge$lambda.min))
print(paste("The 1sd err method lambda",cv_ridge$lambda.1se))

# Plotting
par(mfrow=c(1,3), mar=c(4,4,4,1), oma=c(0.5,0.5,0.5,0))
plot(ridge_fit, xvar = "lambda", label = T)
plot(cv_ridge)
plot(ridge_fit, xvar = "lambda", label = T) 
abline(v = log(cv_ridge$lambda.1se))
```

```{r ridgePlot, echo=FALSE, fig.cap="\\label{fig:ridge}Model selection for ridge shrinkage", fig.width=8} 
par(mfrow=c(1,2))
# cross validation plot 
plot(cv_ridge)
# Plot for report
plot(ridge_fit, xvar = "lambda", label = T) 
abline(v = log(cv_ridge$lambda.1se))
```

In Figure \ref{fig:ridge} on the left, the cross validation for different values of the regularization parameter $\lambda$ is shown. On the right, the coefficients for the individual covariates are depicted agains $\log\lambda$, where the optimal $\lambda$-choice is highlighted. As typical for ridge, the coefficients are shrinked towards 0, but all parameters remain positive weights. This makes the outcome still hard to interpret for our practical data set at hand.


## Lasso

In contrast to the previous ridge regression, the lasso adds $L_1$-regularisation to the regression problem. As before, the implemenation in R relies on the `glmnet` library and the hyperparameter $\lambda$ is tuned as aforementioned.

```{r lasso, eval=TRUE, include=FALSE, fig.height=5, fig.width=15}
# Import glmnet library for lasso
library(glmnet)

# Model generation
gen_mod <- glmnet(x=x_mod,y=y_mod,alpha=1,standardize=TRUE)
gen_lambda_mod <- gen_mod$lambda
gen_lambda_mod

plot(gen_mod,xvar="lambda",label=TRUE)
plot(gen_mod) #plot against the L1 norm

#Choosing optimal lambda performing Crossvalidation
cv_gen_mod=cv.glmnet(x=x_mod,y=y_mod,alpha=1)
print(paste("The lamda giving the smallest CV error",cv_gen_mod$lambda.min))
print(paste("The 1sd err method lambda",cv_gen_mod$lambda.1se))
plot(cv_gen_mod)

#use 1sd error rule default
plot(gen_mod,xvar="lambda",label=TRUE)
abline(v=log(cv_gen_mod$lambda.1se))

coef(gen_mod, s= cv_gen_mod$lambda.1se)
results_mod=cbind(coef(gen_mod,s=cv_gen_mod$lambda.1se),coef(lm_mod))
colnames(results_mod)=c("general lasso","vanilla LS")
print(results_mod)
```

```{r lassoPlot, echo=FALSE, fig.cap="\\label{fig:lasso}Model selection for lasso shrinkage", fig.width=8}
par(mfrow=c(1,2))
# cross validation plot 
plot(cv_gen_mod)
# Plot for report
plot(gen_mod,xvar="lambda",label=TRUE)
abline(v=log(cv_gen_mod$lambda.1se))
```

In Figure \ref{fig:lasso} on the left, the cross validation for different values of the regularization parameter $\lambda$ is shown. On the right, we again see the model coefficients of the covariate set plotted against $\log\lambda$ where the optimal $\lambda$ chosen by cross validation and the decision rule in ELS is highlighted. Finally and as expected, some coefficients are shrinked to 0. However for the optimal $\lambda$, some levels of the construction years `bj` and some of the districts `bez` are shrinked to 0 and other would be still significant. For the practical model problem, this is a non-intuitive behaviour. 


## Group lasso

The group lasso allows to gather some of the covariates and treat those with the same coefficient jointly in the $L_1$-regularised problem. For the implementation in R we utilize the `grplasso` library. Naturally, we group the different levels of the factorial variables, i.e. the years of construction `bj` and the different levels for the districts `bez` together, respectively. 

```{r groupLasso, eval=TRUE, include=FALSE}
# Load library for group lasso
library(grplasso)

# Preparation of data frame
# Adding an intercept to the design matrix, because of new package now used
x_mod_group <- cbind(1, x_mod)
colnames(x_mod_group)[1] <- c("Intercept")
df_mod_group <- data.frame(y_mod,x_mod_group)
colnames(df_mod_group)[1] <-c("nm")

# Model definition
district <- rep(4,24) #25 levels of factor bez (1 level goes into intercept) 
year <- rep(3,43) #44 levels of factor bj (1 level goes itno intecept)
index_mod <- c(NA,1,2,year,district,5,6,7,8,9,10,11) #individual groups for all except bez and bj

grp_lambda_mod <- lambdamax(x=x_mod_group, y = y_mod, index = index_mod, penscale = sqrt,
                          model = LinReg(),standardize = TRUE,center = TRUE)*0.5^(0:40)
grp_lambda_mod <- grp_lambda_mod[c(3:(length(grp_lambda_mod)-10))]

# fit using grouped index
grp_mod <- grplasso(x=x_mod_group, y = y_mod, index = index_mod, lambda = grp_lambda_mod[-(1:3)], model = LinReg(),
                             penscale = sqrt,
                             control = grpl.control(update.hess = "lambda", trace = 0))

## Plot coefficient paths
plot(grp_mod)
str(grp_mod)
coef(grp_mod)

```

Again we employ the same model selection criterion via cross validation as above, but as `grplasso` does not contain a built in cv function of our knowledge, we implement it in R using the `gglasso` library. In the cross validation procedure, we can observe sudden jumps when a new group is included or shrinked from the model. Since the calculatations either include all or none of the levels of the factorial variables, this can lead to a jump in the number of parameters from 9 to 53 in a single step on the lambda grid. 

```{r groupLassoCV, eval=T, include=F}
# Load gglasso library for cross validation
library(gglasso)
# Use own defined lambdagrid
fitls_mod <- gglasso(x = x_mod_group[,-1], y = y_mod, group = index_mod[-1], loss = "ls",lambda=grp_lambda_mod)
plot(fitls_mod)

# crossvalidation on self defined lambdagrid 
# WARNING: this takes some time
cvfitls_mod <- cv.gglasso(x = x_mod_group[,-1], y = y_mod, group = index_mod[-1], loss = "ls",lambda=grp_lambda_mod)
plot(cvfitls_mod)
str(cvfitls_mod)
cvfitls_mod$lambda.1se

#fit with optimal lambda on defined lambda grid(either package made or made by me)
fin_mod=gglasso(x = x_mod_group[,-1], y = y_mod, group = index_mod[-1], loss = "ls",lambda=exp(cvfitls_mod$lambda.1se))
coef(fin_mod)
```

```{r groupLassoPlot, echo=FALSE, fig.cap="\\label{fig:group}Model selection for group lasso", fig.width=8}
par(mfrow=c(1,2))
# cross validation plot (by package description loss="ls" is mean-squared error) 
plot(cvfitls_mod)
# Plot for report
plot(fitls_mod)
abline(v=(cvfitls_mod$lambda.1se))
```

In Figure \ref{fig:group} on the left, the cross validation for different values of the regularization parameter $\lambda$ is shown. (NB, in contrast to the previous packages, `gglasso` does not show `nzeros`, the number of active covariates per $\lambda$, in the head line above the plot) The jumps when a new group of covariates are included in the model are clearly visible. On the right, we again depict the coefficients of the covariates against the $\log\lambda$ and highlight the optimal hyperparameter. By construction, all levels of a factorial variable are shrinked to zero simultaneously. This means that either all levels remain in the model or all levels are excluded, which corresponds better to the practical interpretation of those variables. 

## Summary

Lastly, we depict a subset of 14 parameter estimates from the methods presented in this report, with most of the grouped covariates for `bez` and `bj` removed for simplicity. 

```{r parSum, eval=TRUE, include=TRUE, echo = FALSE}
#final results, group lasso either shrinks all or nothing and even increases some estimated parameter coefficients
results_mod=cbind(coef(lm_mod), coef(ridge_fit,s=cv_ridge$lambda.1se), coef(gen_mod,s=cv_gen_mod$lambda.1se), coef(fin_mod))
colnames(results_mod)=c("vanilla LS", "ridge", "general lasso", "group lasso")

#print a subset of results
print(results_mod[c(1,2,3,43,44,60,61,71:77),])
```


By including two estimated parameters from levels of `bez` and `bj`, we demonstrate the non-intuitive behaviour of the general lasso. We observe that the `bez` and `bj` parameters not shrinked by the general lasso, corresponds to the highly significant covariates in the vanilla LS. Furthermore, there is a continuous shrinkage of the grouped covariates, starting from vanilla LS, shrinked in rigde and then further in general lasso, before finally resulting in zero for the grouped lasso. These trends are repeated for all parameter estimates in `bez` and `bj`, including the ones not listed above. 

For the non-grouped covariates however, the shrinkage is moving in a bumpy pace, by sometimes being shrinked and other times in fact increased when compared to the Vanilla LS. This might be explained by the fact that there is correlation between the variables, and by shrinking the grouped covariates, the non grouped covariates contain more of the variability of the data set for the optimal lambda. 

# Inference

## Can we afford a test set?

The data set is presumably too small to divide it into a training and a test set. We investigate this assumption by pretending an 80/20 training-test split for 100 different random splittings and keeping track of the response mean of the test set. 

```{r dataSplitting, eval=T, include=F}
B = 100 # number of compared random splits
a = 0.8 # splitting ratio for training set 

split_means = rep(0, B)
set.seed(1)
for (b in 1:B){
  # Generate a random selection of training indices 
  split_idxs  <- sample(seq_len(nrow(munich_house)), size = floor(0.8*nrow(munich_house)))
  # Separate the corresponding test set
  split_test  <- munich_house[-split_idxs, "nm"]
  # Store the test mean
  split_means[b] <- mean(split_test)
}

var(split_means)
```
The variance of the test mean is more than 121, which means that depending on the split huge variations in the test data would be introduced, whereby the test data cannot be used for a proper representation of the original data set. This justifies why we cannot afford a split.


## Bootstrapping

As seen before, we cannot afford a split into test and training set for our data set, therefore we use bootstrapping for inference. Bootstrapping assigns measures of accuracy to samples of estimates, hence we use it here as a way of validation. 

```{r,echo = F, eval=T, tidy = F, fig.width=16, fig.height=5, fig.cap="\\label{fig:boxplot_lasso} Boxplot of the bootstrapped general lasso coefficients"}
# ```{r,echo = F, eval=T, tidy = F}
# boostrap loop
set.seed(2021)
B=100
n=nrow(x_mod)
p=ncol(x_mod)
lassomat=matrix(ncol=p+1,nrow=B)
grplassomat=matrix(ncol=p+1,nrow=B)
library(grplasso)
library(gglasso)
library(ggplot2)
library(glmnet)
# no need or separate function for steps 1-6 since can use cv.glmnet
# and weight argument for giving the new bootstrapped data
for (b in 1:B)
{
  ids=sort(sample(1:n,replace=TRUE))
  wids=rep(0,n)
  for (i in 1:n)
    wids[i]=sum(ids==i)
  resl=cv.glmnet(x_mod,y_mod,weights=wids)
  regrp=gglasso(x = x_mod_group[ids,-1], y = y_mod[ids], group = index_mod[-1], loss = "ls",lambda=exp(cvfitls_mod$lambda.1se)) # gglasso

  lassomat[b,]=as.vector(coef(resl)) 
  grplassomat[b,]=as.vector(coef(regrp))
}
colnames(grplassomat)=c("Int.cept",colnames(x_mod))
colnames(lassomat)=c("Int.cept",colnames(x_mod))

# plotting boxplots
lassomatUI=lassomat[,-1]
lassods=reshape2::melt(lassomatUI,
         variable.name ="variable",value.name="value")
lassopp=ggplot(lassods,aes(x=Var2,y=value))+
  geom_boxplot()+ggtitle("")
print(lassopp)

```

```{r, echo = F, eval=T, tidy = F, fig.width=15, fig.height=5, fig.cap="\\label{fig:boxplot_grplasso} Boxplot of the bootstrapped group lasso selected coefficients"}
# In this section, some of the chosen variables to be plotted, to avoid the scaling from R to ignore the box
grplassomatUI=grplassomat[,-c(1,25:65)]
grplassods=reshape2::melt(grplassomatUI,
         variable.name ="variable",value.name="value")
grplassopp=ggplot(grplassods,aes(x=Var2,y=value))+
  geom_boxplot()+ggtitle("")
print(grplassopp)

```

The general lasso coefficients as depicted in Fig. \ref{fig:boxplot_lasso} shows a huge variability for estimated parameters corresponding to the factorial variables for their different levels. Here, we can again see that the split into training and test data would be risky. In contrast to this, the group lasso handles `bez` and `bj` as respective groups and they are always assigned the same parameter. As it can be seen in Fig. \ref{fig:boxplot_grplasso}, the group lasso always shrinks the corresponding coefficients of the factorial variables all to zero. Note that we left out some grouped covariates since they are anyway zeros and thereby, the plot becomes more readable. Moreover, the variability of the parameters for the non-grouped variables is not much increased. It coincides with our previous argument that the group lasso is particularly suited for our data set with its factorial variables. 


## Conclusion

```{r final, eval=TRUE, include=FALSE}
#final results, group lasso either shrinks all or nothing and even increases some estimated parameter coefficients
results_mod=cbind(coef(lm_mod), coef(ridge_fit,s=cv_ridge$lambda.1se), coef(gen_mod,s=cv_gen_mod$lambda.1se), coef(fin_mod))
colnames(results_mod)=c("vanilla LS", "ridge", "general lasso", "group lasso")
print(results_mod)
```

In this project, we have chosen a practical data set which contains data on rental prices in Munich (two of the group members are practically familiar with the difficult housing situation in Munich and it was appealing to analyse this statistically). For the data analysis, the factorial variables needed special attention. A plain vanilla and ridge regression were not capable to give explainable outcomes. Likewise, the result of the lasso was contra-intuitive in the unclear handling of the factorial varibles. Finally, the group lasso where a factorial variable can be arranged together leads to an interpretable shrinkage conclusion, where all variables except the factorial are selected for the optimal hyperparemeter choice. The bootstrapping validates our assumption that group lasso the preferred shrinkage method leading to the most interpretable and robust results. 




