---
title: "Data Analysis Project 1"
author: 'Group 5 : Yellow Submarine'
date: "`r format(Sys.time(), '15 February, 2021')`"
output:
  pdf_document: default
  html_document: default
subtitle: MA8701
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
```

In this project, we analyse a real dataset using shrinkage methods from part 1 of the MA8701 course.

### Note on Open Science

To pursue the idea of reproducible research, the chosen dataset as well as the code for our analysis are publicly accessible:

* dataset: https://data.ub.uni-muenchen.de/2/1/miete03.asc
* code: https://github.com/FlorianBeiser/MA8701

```{r rpackages, eval=TRUE, include=FALSE}
#load packages
library(GGally)
library(tidyverse)
library(corrplot)
```


# The Data Set 

For our project work we use the Munich Rent 2003 data set as described in https://rdrr.io/cran/LinRegInteractive/man/munichrent03.html. The data set has 12 original covariates, where a brief introduction to these parameters is listed below (in brackets the type of the covariate is explicated), and 2053 observations are available:

-   `nmqm`: rent per square meter (double)
-   `wfl`: area in square meters (int)
-   `rooms`: number of rooms (int)
-   `bj`: year of construction (Factor)
-   `bez`: district (Factor)
-   `wohngut`: quality of location (int)
-   `wohnbest`: high quality of location (int)
-   `ww0`: hot water supply available (int)
-   `zh0`: central heating (int)
-   `badkach0`: tiled bathroom (int)
-   `badextra`: high-quality bathroom (int)
-   `kueche`: upscale kitchen equipment (int)

and the response

-   `nm`: rental price (double).

The label "double" naturally stands for numerical values, "int" categorizes parameters with integer values, and "Factor" symbolize parameter taking a certain number of levels - where in contrast to integers a higher level does not necessarily mean an improvement. 

```{r loadData, eval=TRUE, include=FALSE}
# Load data set
munich_house <- read.table(
  "https://data.ub.uni-muenchen.de/2/1/miete03.asc",
  sep="\t", header=TRUE)

```

Since the price per square meter `nmqm` multiplied with the area `wfl` directly gives the rental price `nm` which we define as the response in the system, it does not make sense to keep both values. Hence, we exclude `nmqm` from the data set to avoid it consuming all the significance in the coming data analysis. 


```{r dataPlot, echo=FALSE, fig.cap="\\label{fig:data}Exploration of multi-colinearity in the data set"} 
# Correlation plot with excluding nmqm, so to find the colinearlity
# Observe correlations between covariates
xs=model.matrix(nm~.-bj -bez,data=munich_house)[,-c(1:2)] # to take care of categorical variables, but not include the intercept column
xss=scale(xs)
corrplot.mixed(cor(xss),upper = "ellipse", lower = "number", tl.cex = 0.5)
```

Figure \ref{fig:data} shows the correlation between covariates ignoring the factorials and reveals that the dataset may suffer from a very light multi-colinearity. 

However, the factorial variable `bj` and `bez` introduce 44 and 25 levels, respectively, leading to relatively unclear dependencies between the full covariate set (including factorial variables) and the response, which makes this dataset suitable for a regression analysis and the application of shrinkage methods. 

```{r buildDataFrame, eval=TRUE, include=FALSE} 
# 
munich_house$bez=as.factor(munich_house$bez)
munich_house$bj=as.factor(munich_house$bj)
str(munich_house)

#create model matrix without intercept and without nmqm
x_mod <- model.matrix(nm~.-nmqm,data=munich_house)[,-1]
y_mod <- munich_house$nm
df_mod <- data.frame(y_mod,x_mod)
colnames(df_mod)[1]="nm"
```


# Data Analysis

Subsequently, we start with a plain linear regression model as reference such that we can particularly point out the benefits of shrinkage approaches. As shrinkage methods, we employ the ridge, the lasso and the group lasso. The latter approach seems to be very well suited for our data set, as it allows to take the factorial variables as a single unit for shrinkage. 


## Regression

We start the data analysis with a vanilla LM regression for reference using R's internal `lm` functionality

```{r regression, eval=TRUE, include=FALSE}
# Vanilla regression model
lm_mod <- lm(nm~.,data=df_mod) 
#Printing the summary to assess significant parameters
summary(lm_mod)
```

The the regression results show a lot of significant covariates. As maybe expected, the area `wfl` is strongly related to the rent price, however confusingly, the significance of different levels of the years of construction `bj` and districts `bez` varies a lot. From those both observations, it is not possible, to extract clear data analysis results, which also would match our interpretation of the problem. 


## Ridge Shrinkage

As first shrinkage method, we consider the ridge regression that uses Tikhonov regularisation in the model, where we utilize the `glmnet` library for its implementation in R. Since ridge introduces the additional tuning parameter $\lambda$ we perform cross validation for the model selection, i.e. for the choice of the optimal $\lambda$, where we follow the advice in the ELS to choose $\lambda$ as the one with minimal CV-error plus one standard deviation of the CV-error. 

```{r ridge, eval=TRUE, include=FALSE}
# Import glmnet library for ridge
library(glmnet)

# Massaged lambda grid
gen_mod_for_lambda <- glmnet(x = x_mod, y = y_mod, standardize=TRUE, alpha = 0)
my_lambda <- gen_mod_for_lambda$lambda
my_lambda <- c(my_lambda, 10, 5, 3, 1, 0.5, 0.1) # add more to approach zero lambda

# Model definition
ridge_fit <- glmnet(x_mod, y_mod, standardize=TRUE, alpha = 0, lambda = my_lambda)

#Choosing optimal lambda performing Crossvalidation
cv_ridge <- cv.glmnet(x_mod, y_mod, standardize=TRUE, alpha = 0, lambda = my_lambda)

print(paste("The lamda giving the smallest CV error",cv_ridge$lambda.min))
print(paste("The 1sd err method lambda",cv_ridge$lambda.1se))

# Plotting
par(mfrow=c(1,3), mar=c(4,4,4,1), oma=c(0.5,0.5,0.5,0))
plot(ridge_fit, xvar = "lambda", label = T)
plot(cv_ridge)
plot(ridge_fit, xvar = "lambda", label = T) 
abline(v = log(cv_ridge$lambda.1se))
```

```{r ridgePlot, echo=FALSE, fig.cap="\\label{fig:ridge}Model selection for ridge shrinkage", fig.width=8} 
par(mfrow=c(1,2))
# cross validation plot 
plot(cv_ridge)
# Plot for report
plot(ridge_fit, xvar = "lambda", label = T) 
abline(v = log(cv_ridge$lambda.1se))
```

In Figure \ref{fig:ridge} on the left, the cross validation for different values of the regularization parameter $\lambda$ is shown. On the right, the coefficients for the individual covariates are depicted agains $\log\lambda$, where the optimal $\lambda$-choice is highlighted. As typical for ridge, the coefficients are shrinked towards 0, but all parameters remain positive weights. This makes the outcome still hard to interpret for our practical data set at hand.


## Lasso

In contrast to the previous ridge regression, the lasso adds $L_1$-regularisation to the regression problem. As before, the implemenation in R relies on the `glmnet` library and the hyperparameter $\lambda$ is tuned as aforementioned.

```{r lasso, eval=TRUE, include=FALSE, fig.height=5, fig.width=15}
# Import glmnet library for lasso
library(glmnet)

# Model generation
gen_mod <- glmnet(x=x_mod,y=y_mod,alpha=1,standardize=TRUE)
gen_lambda_mod <- gen_mod$lambda
gen_lambda_mod

plot(gen_mod,xvar="lambda",label=TRUE)
plot(gen_mod) #plot against the L1 norm

#Choosing optimal lambda performing Crossvalidation
cv_gen_mod=cv.glmnet(x=x_mod,y=y_mod,alpha=1)
print(paste("The lamda giving the smallest CV error",cv_gen_mod$lambda.min))
print(paste("The 1sd err method lambda",cv_gen_mod$lambda.1se))
plot(cv_gen_mod)

#use 1sd error rule default
plot(gen_mod,xvar="lambda",label=TRUE)
abline(v=log(cv_gen_mod$lambda.1se))

coef(gen_mod, s= cv_gen_mod$lambda.1se)
results_mod=cbind(coef(gen_mod,s=cv_gen_mod$lambda.1se),coef(lm_mod))
colnames(results_mod)=c("general lasso","vanilla LS")
print(results_mod)
```

```{r lassoPlot, echo=FALSE, fig.cap="\\label{fig:lasso}Model selection for lasso shrinkage", fig.width=8}
par(mfrow=c(1,2))
# cross validation plot 
plot(cv_gen_mod)
# Plot for report
plot(gen_mod,xvar="lambda",label=TRUE)
abline(v=log(cv_gen_mod$lambda.1se))
```

In Figure \ref{fig:lasso} on the left, the cross validation for different values of the regularization parameter $\lambda$ is shown. On the right, we again see the model coefficients of the covariate set plotted against $\log\lambda$ where the optimal $\lambda$ chosen by cross validation and the decision rule in ELS is highlighted. Finally and as expected, some coefficients are shrinked to 0. However for the optimal $\lambda$, some levels of the construction years `bj` and some of the districts `bez` are shrinked to 0 and other would be still significant. For the practical model problem, this is a non-intuitive behaviour. 


## Group lasso

The group lasso allows to gather some of the covariates and treat those with the same coefficient jointly in the $L_1$-regularised problem. For the implementation in R we utilize the `grplasso` library. Naturally, we group the different levels of the factorial variables, i.e. the years of construction `bj` and the different levels for the districts `bez` together, respectively. 

```{r groupLasso, eval=TRUE, include=FALSE}
# Load library for group lasso
library(grplasso)

# Preparation of data frame
# Adding an intercept to the design matrix, because of new package now used
x_mod_group <- cbind(1, x_mod)
colnames(x_mod_group)[1] <- c("Intercept")
df_mod_group <- data.frame(y_mod,x_mod_group)
colnames(df_mod_group)[1] <-c("nm")

# Model definition
district <- rep(4,24) #25 levels of factor bez (1 level goes into intercept) 
year <- rep(3,43) #44 levels of factor bj (1 level goes itno intecept)
index_mod <- c(NA,1,2,year,district,5,6,7,8,9,10,11) #individual groups for all except bez and bj

grp_lambda_mod <- lambdamax(x=x_mod_group, y = y_mod, index = index_mod, penscale = sqrt,
                          model = LinReg(),standardize = TRUE,center = TRUE)*0.5^(0:40)
grp_lambda_mod <- grp_lambda_mod[c(3:(length(grp_lambda_mod)-10))]

# fit using grouped index
grp_mod <- grplasso(x=x_mod_group, y = y_mod, index = index_mod, lambda = grp_lambda_mod[-(1:3)], model = LinReg(),
                             penscale = sqrt,
                             control = grpl.control(update.hess = "lambda", trace = 0))

## Plot coefficient paths
plot(grp_mod)
str(grp_mod)
coef(grp_mod)

```

Again we employ the same model selection criterion via cross validation as above, but as `grplasso` does not contain a built in cv function of our knowledge, we implement it in R using the `gglasso` library. In the cross validation procedure, we can observe sudden jumps when a new group is included or shrinked from the model. Since the calculatations either include all or none of the levels of the factorial variables, this can lead to a jump in the number of parameters from 9 to 53 in a single step on the lambda grid. 

```{r groupLassoCV, eval=T, include=F}
# Load gglasso library for cross validation
library(gglasso)
# Use own defined lambdagrid
fitls_mod <- gglasso(x = x_mod_group[,-1], y = y_mod, group = index_mod[-1], loss = "ls",lambda=grp_lambda_mod)
plot(fitls_mod)

# crossvalidation on self defined lambdagrid 
# WARNING: this takes some time
cvfitls_mod <- cv.gglasso(x = x_mod_group[,-1], y = y_mod, group = index_mod[-1], loss = "ls",lambda=grp_lambda_mod)
plot(cvfitls_mod)
str(cvfitls_mod)
cvfitls_mod$lambda.1se

#fit with optimal lambda on defined lambda grid(either package made or made by me)
fin_mod=gglasso(x = x_mod_group[,-1], y = y_mod, group = index_mod[-1], loss = "ls",lambda=exp(cvfitls_mod$lambda.1se))
coef(fin_mod)
```

```{r groupLassoPlot, echo=FALSE, fig.cap="\\label{fig:group}Model selection for group lasso", fig.width=8}
par(mfrow=c(1,2))
# cross validation plot (by package description loss="ls" is mean-squared error) 
plot(cvfitls_mod)
# Plot for report
plot(fitls_mod)
abline(v=(cvfitls_mod$lambda.1se))
```

In Figure \ref{fig:group} on the left, the cross validation for different values of the regularization parameter $\lambda$ is shown. (NB, in contrast to the previous packages, `gglasso` does not show `nzeros`, the number of active covariates per $\lambda$, in the head line above the plot) The jumps when a new group of covariates are included in the model are clearly visible. On the right, we again depict the coefficients of the covariates against the $\log\lambda$ and highlight the optimal hyperparameter. By construction, all levels of a factorial variable are shrinked to zero simultaneously. This means that either all levels remain in the model or all levels are excluded, which corresponds better to the practical interpretation of those variables. 

## Summary


```{r parSum, eval=TRUE, include=TRUE, echo = FALSE}
#final results, group lasso either shrinks all or nothing and even increases some estimated parameter coefficients
results_mod=cbind(coef(lm_mod), coef(ridge_fit,s=cv_ridge$lambda.1se), coef(gen_mod,s=cv_gen_mod$lambda.1se), coef(fin_mod))
colnames(results_mod)=c("vanilla LS", "ridge", "general lasso", "group lasso")
#Compare the results with the significant cov in lm 
#print(results_mod)
#summary(lm_mod)

#print a subset of results
print(results_mod[c(1,2,3,43,44,60,61,71:77),])
```

Lastly, we depict a subset of 14 parameter estimates from the methods presented in this report, with most of the grouped covariates for `bez` and `bj` removed for simplicity. 

By including two estimated parameters from `bez` and `bj`, we demonstrate the non-intuitive behaviour of the general lasso. We observe that the `bez` and `bj` parameters not shrinked by the general lasso, corresponds to the highly significant covariates in the vanilla LS. Furthermore, there is a continuous shrinkage of the grouped covariates, starting from vanilla LS, shrinked in rigde and then further in general lasso, before finally resulting in zero for the grouped lasso. These trends are repeated for all parameter estimates in `bez` and `bj`, including the ones not listed above. 

For the non grouped covariates however, the shrinkage is moving in a bumpy pace, by sometimes being shrinked and other times in fact increased when compared to the Vanilla LS.

@HMO How to explain this last part???? Do we need to include it? I am a true believer of the less you know the less you say

# Inference

## Can we afford a test set?

The data set is presumably too small to divide it into a training and a test set. We investigate this assumption by pretending an 80/20 training-test split for 100 different random splittings and keeping track of the response mean of the test set. 

```{r dataSplitting, eval=T, include=F}
B = 100 # number of compared random splits
a = 0.8 # splitting ratio for training set 

split_means = rep(0, B)
set.seed(1)
for (b in 1:B){
  # Generate a random selection of training indices 
  split_idxs  <- sample(seq_len(nrow(munich_house)), size = floor(0.8*nrow(munich_house)))
  # Separate the corresponding test set
  split_test  <- munich_house[-split_idxs, "nm"]
  # Store the test mean
  split_means[b] <- mean(split_test)
}

var(split_means)
```
The variance of the test mean is more than 121, which means that depending on the split huge variations in the test data would be introduced, whereby the test data cannot be used for a proper representation of the original data set. This justifies why we cannot afford a split.


## Bootstrapping

As seen before, we cannot afford a split into test and training set for our data set, therefore we use bootstrapping for inference. Bootstrap can be applied here to find the proportion of times each covariate is shrinked to zero.  So it is a way of validation. 

```{r,echo = F, eval=T, tidy = F, fig.width=16, fig.height=5, fig.cap="\\label{fig:boxplot_lasso} Boxplot of the bootstrapped general lasso coefficients"}
# ```{r,echo = F, eval=T, tidy = F}
# boostrap loop
set.seed(2021)
B=100
n=nrow(x_mod)
p=ncol(x_mod)
lassomat=matrix(ncol=p+1,nrow=B)
ridgemat=matrix(ncol=p+1,nrow=B)
grplassomat=matrix(ncol=p+1,nrow=B)
library(grplasso)
library(gglasso)
library(ggplot2)
library(glmnet)
# no need or separate function for steps 1-6 since can use cv.glmnet
# and weight argument for giving the new bootstrapped data
for (b in 1:B)
{
  ids=sort(sample(1:n,replace=TRUE))
  wids=rep(0,n)
  for (i in 1:n)
    wids[i]=sum(ids==i)
  resl=cv.glmnet(x_mod,y_mod,weights=wids)
  resr=cv.glmnet(x_mod,y_mod,weights=wids,alpha=0)
  regrp=gglasso(x = x_mod_group[ids,-1], y = y_mod[ids], group = index_mod[-1], loss = "ls",lambda=exp(cvfitls_mod$lambda.1se)) # gglasso

  lassomat[b,]=as.vector(coef(resl)) 
  ridgemat[b,]=as.vector(coef(resr)) 
  grplassomat[b,]=as.vector(coef(regrp))
}
colnames(grplassomat)=colnames(ridgemat)=c("Int.cept",colnames(x_mod))
colnames(lassomat)=colnames(ridgemat)=c("Int.cept",colnames(x_mod))

# plotting boxplots
lassomatUI=lassomat[,-1]
lassods=reshape2::melt(lassomatUI,
         variable.name ="variable",value.name="value")
lassopp=ggplot(lassods,aes(x=Var2,y=value))+
  geom_boxplot()+ggtitle("")
print(lassopp)

grplassomatUI=grplassomat[,-1]
grplassods=reshape2::melt(grplassomatUI,
         variable.name ="variable",value.name="value")
grplassopp=ggplot(grplassods,aes(x=Var2,y=value))+
  geom_boxplot()+ggtitle("")

ridgematUI=ridgemat[,-1]
ridgeds=reshape2::melt(ridgematUI,variable.name="variable",value.name="value")
ridgepp=ggplot(ridgeds,aes(x=Var2,y=value))+
  geom_boxplot()+ggtitle("")
```

```{r, echo = F, eval=T, tidy = F, fig.width=15, fig.height=5, fig.cap="\\label{fig:boxplot_grplasso} Boxplot of the bootstrapped group lasso selected coefficients"}
# In this section, some of the chosen variables to be plotted, to avoid the scaling from R to ignore the box
var_selected = c(2, 3, 28:42, 70:76)
n_selected = length(var_selected)
val_selected_vec = matrix(0, 100 * n_selected, 3)
val_selected = matrix(0, 100, n_selected)

col_names = list()
j = 0
for (i in var_selected){
  j = j + 1
  val_selected[, j] = grplassomat[, i]
  col_names = c(col_names, colnames(x_mod)[i])
}
colnames(val_selected) = col_names
grplassods=reshape2::melt(val_selected,
         variable.name ="variable",value.name="value")
grplassopp=ggplot(grplassods,aes(x=Var2,y=value))+
  geom_boxplot()+ggtitle("")
print(grplassopp)
```

The boxplot \ref{fig:boxplot_lasso} and \ref{fig:boxplot_grplasso} validates our motivation to use group lasso. The general lasso coefficients as depicted in \ref{fig:boxplot_lasso} shows a huge variablity in the factorial variable regions, where `bj` and `bez` have large span over the coefficient range. While the group lasso as in \ref{fig:boxplot_grplasso} shrinks the corresponding coefficients to zero. It coincides with the efficacy of using grouplasso for this specific dataset with mulitple factorial covariates. Since many of those coefficients are shrinked to zeros in the group lasso boxplot, as opposed to the general lasso boxplot, thereby it is logical to only include some of the coefficients where they show huge variation in the counterpart general lasso boxplot for the factorial covariates `bj` and `bez` as a comparison.

So to conclude, one can tell group lasso seems working suitably for this specific case where its categorical covariates have many groups. Ridge regression does not shrink any parameters dramatically while lasso does a bit on the shrinkage, but the most shrinked contributation is from group lasso.


> @YG: what happened to the grouped covariates always being shrinked to zero in the histogram you showed last meeting?? This does not make sense, Please explain this tomorrow!What are you validating? The ridge, general lasso or the grouped one? Or are you validating them all at the same time? I'm not a bootstrap expert but I am lost, so please include an explanation!

> YG: I am using `lasso`, which is simple, `grplasso` always give the same coefficients, I guess if it is converged, i.e., when we choose the 1se lambda, which shrinks the right part of the coefficients. Therefore, no matter how we change the dataset, it always yields the same coefficients. But I don't know, maybe we should recheck again the implementation from the grouplasso part. I directly used the one in the group lasso section.

> @YG: Using only lasso in this section contradicts to the whole narrative of this report!! You can again include it but group lasso has to be there as well!

> @YG: Remember to keep the code clean before submission.
>YG: code is clean, git is not.
>YG: I think lasso did the work of visualising the purpose of small dataset, i.e., it is too various when looping through, therefore, we have a small dataset. If grouplasso is correct, we can again plot them. But agree on which form to use. But as for validataion in the inference part, it suffices to show the varibility. As for method validation, I think it is true as what Mette said, we can try to use different lasso variants. From the boxplot, it is not that interesting to see anything, since it is very stable or no variation. 

>YG: Added grouplasso bootstrap, it coincide with the conclusions now. But only full boxplot can reveal the full picture of the group lasso, thus full boxplot is shown.

## Conclusion

```{r final, eval=TRUE, include=FALSE}
#final results, group lasso either shrinks all or nothing and even increases some estimated parameter coefficients
results_mod=cbind(coef(lm_mod), coef(ridge_fit,s=cv_ridge$lambda.1se), coef(gen_mod,s=cv_gen_mod$lambda.1se), coef(fin_mod))
colnames(results_mod)=c("vanilla LS", "ridge", "general lasso", "group lasso")
print(results_mod)
```

In this project, we have chosen a practical data set which contains data on rental prices in Munich (two of the group members are practically familiar with the difficult housing situation in Munich and it was appealing to analyse this statistically). For the data analysis, the factorial variables needed special attention. A plain vanilla and ridge regression were not capable to give explainable outcomes. Likewise, the result of the lasso was contra-intuitive in the unclear handling of the factorial varibles. Finally, the group lasso where a factorial variable can be arranged together leads to an interpretable shrinkage conclusion, where all variables except the factorial are selected for the optimal hyperparemeter choice. From the bootstrapping validation, it can be concluded that the variability of the coefficients seem to be high enough which hinder the implementation of a test set. 


