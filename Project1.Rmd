---
subtitle: "MA8701"
title: "Data Analysis Project 1"
author: "Group 5 : Yellow Submarine"
date: "`r format(Sys.time(), '15 February, 2021')`"
output: html_document
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
```

In this project, we analyse a real dataset using shrinkage methods from part 1 of the MA8701 course.

### Note on Open Science

To pursue the idea of reproducible research, the chosen dataset as well as the code for our analysis are publicly accessible:

* dataset: https://data.ub.uni-muenchen.de/2/1/miete03.asc
* code: https://github.com/FlorianBeiser/MA8701

```{r rpackages, eval=TRUE, include=FALSE}
#load packages
library(GGally)
library(tidyverse)
```


# The Data Set 

For our project work we use the Munich Rent 2003 data set as described in https://rdrr.io/cran/LinRegInteractive/man/munichrent03.html. The data set has 12 original covariates, where a brief introduction to these parameters is listed below (in brackets the type of the covariate is explicated), and 2053 observations are available:

-   `nmqm`: rent per square meter (double)
-   `wfl`: area in square meters (int)
-   `rooms`: number of rooms (int)
-   `bj`: year of construction (Factor)
-   `bez`: district (Factor)
-   `wohngut`: quality of location (int)
-   `wohnbest`: high quality of location (int)
-   `ww0`: hot water supply available (int)
-   `zh0`: central heating (int)
-   `badkach0`: tiled bathroom (int)
-   `badextra`: high-quality bathroom (int)
-   `kueche`: upscale kitchen equipment (int)
and the response
-   `nm`: rental price (double).

"Double" naturally stands for numerical values, "int" categories parameters with integer values, and "Factor" symbolize parameter taking a certain number of levels.

```{r loadData, eval=TRUE, include=FALSE}
# Load data set
munich_house <- read.table(
  "https://data.ub.uni-muenchen.de/2/1/miete03.asc",
  sep="\t", header=TRUE)

munich_house$bez=as.factor(munich_house$bez)
munich_house$bj=as.factor(munich_house$bj)
str(munich_house)
```

Since the price per square meter `nmqm` together with the area `wfl` directly give the response in the system, we exclude `nmqm` from the data frame to be able to do serious data analysis on the dataset. (@HMO check: should we mention something about unclear signs of multicolinearity) The factorial variable `bj` and `bez` introduce 44 and 25 levels, respectively, leading to relatively unclear dependencies between the full covariate set (with factorial variables) and the response, which makes this dataset suitable for a regression analysis and the application of shrinkage methods. 

```{r buildDataFrame, eval=TRUE, include=FALSE} 
#create model matrix without intercept and without nmqm
x_mod <- model.matrix(nm~.-nmqm,data=munich_house)[,-1]
y_mod <- munich_house$nm
df_mod <- data.frame(y_mod,x_mod)
colnames(df_mod)[1]="nm"
```


# Data Analysis

Subsequently, we start with a plain linear regression model as reference such that we can particularly point out the benefits of shrinkage approaches. As shrinkage methods, we employ the lasso and the group lasso.


## Regression

We start the data analysis with a vanilla LM regression for reference using R's internal `lm` functionality

```{r regression, eval=TRUE, include=FALSE}
# Vanilla regression model
lm_mod <- lm(nm~.,data=df_mod) 
#Printing the summary to assess significant parameters
summary(lm_mod)
```

The the regression results show a lot of significant covariates. As maybe expected, the area `wfl` is strongly related to the rent price, however confusingly, the significance of different levels of the years of construction `bj` and districts `bez` varies a lot. From those both observations, it is not possible, to extract clear data analysis results, which also would match our interpretation of the problem. (@FB: better formulation)


## Ridge Shrinkage

As first shrinkage method, we consider the ridge regression that uses Tikhonov regularisation in the model, where we utilize the `glmnet` library for its implementation in R. Since ridge introduces the additional tuning parameter $\lambda$ we perform cross validation for the model selection, i.e. for the choice of the optimal $\lambda$, where we follow the advice in the ELS to choose $\lambda$ as the one with minimal CV-error plus one standard deviation of the CV-error. 

```{r ridge, eval=TRUE, include=FALSE}
# Import glmnet library for ridge
library(glmnet)

# Massaged lambda grid
gen_mod_for_lambda <- glmnet(x = x_mod, y = y_mod, standardize=TRUE, alpha = 0)
my_lambda <- gen_mod_for_lambda$lambda
my_lambda <- c(my_lambda, 10, 5, 3, 1, 0.5, 0.1) # add more to approach zero lambda

# Model definition
ridge_fit <- glmnet(x_mod, y_mod, standardize=TRUE, alpha = 0, lambda = my_lambda)

#Choosing optimal lambda performing Crossvalidation
cv_ridge <- cv.glmnet(x_mod, y_mod, standardize=TRUE, alpha = 0, lambda = my_lambda)

print(paste("The lamda giving the smallest CV error",cv_ridge$lambda.min))
print(paste("The 1sd err method lambda",cv_ridge$lambda.1se))

# Plotting
par(mfrow=c(1,3), mar=c(4,4,4,1), oma=c(0.5,0.5,0.5,0))
plot(ridge_fit, xvar = "lambda", label = T)
plot(cv_ridge)
plot(ridge_fit, xvar = "lambda", label = T) 
abline(v = log(cv_ridge$lambda.1se))
```

```{r ridgePlot, echo=FALSE, fig.cap="\\label{fig:ridge}Model selection for ridge shrinkage"} 
# Plot for report
plot(ridge_fit, xvar = "lambda", label = T) 
abline(v = log(cv_ridge$lambda.1se))
```

In Figure \ref{fig:ridge}, the coefficients for the individual covariates are depicted agains $\log\lambda$, where the optimal $\lambda$ choice is highlighted. As typical for ridge, the coefficients are shrinked towards 0, but all parameters remain positive weights. This makes the outcome still hard to interpret for our practical data set at hand.


## Lasso

In contrast to the previous ridge regression, the lasso adds $L_1$-regularisation to the regression problem. As before, the implemenation in R relies on the `glmnet` library and the hyperparameter $\lambda$ is tuned as aforementioned.

```{r lasso, eval=TRUE, include=FALSE, fig.height=5, fig.width=15}
# Import glmnet library for lasso
library(glmnet)

# Model generation
gen_mod <- glmnet(x=x_mod,y=y_mod,alpha=1,standardize=TRUE)
gen_lambda_mod <- gen_mod$lambda
gen_lambda_mod

plot(gen_mod,xvar="lambda",label=TRUE)
plot(gen_mod) #plot against the L1 norm

#Choosing optimal lambda performing Crossvalidation
cv_gen_mod=cv.glmnet(x=x_mod,y=y_mod,alpha=1)
print(paste("The lamda giving the smallest CV error",cv_gen_mod$lambda.min))
print(paste("The 1sd err method lambda",cv_gen_mod$lambda.1se))
plot(cv_gen_mod)

#use 1sd error rule default
plot(gen_mod,xvar="lambda",label=TRUE)
abline(v=log(cv_gen_mod$lambda.1se))

coef(gen_mod, s= cv_gen_mod$lambda.1se)
results_mod=cbind(coef(gen_mod,s=cv_gen_mod$lambda.1se),coef(lm_mod))
colnames(results_mod)=c("general lasso","vanilla LS")
print(results_mod)
```

```{r lassoPlot, echo=FALSE, fig.cap="\\label{fig:lasso}Model selection for lasso shrinkage"}
plot(gen_mod,xvar="lambda",label=TRUE)
abline(v=log(cv_gen_mod$lambda.1se))
```

In Figure \ref{fig:lasso}, we again see the model coefficients of the covariate set plotted against $\log\lambda$ where the optimal $\lambda$ chosen by cross validation and the decision rule in ELS in highlighted. Finally and as expected, some coeffiecients are shrinked to 0. However for the optimal $\lambda$, some levels of the construction years `bj` and some of the districts `bez` are shrinked to 0 and other would be still significant. For the practical model problem, this is a non-intuitive behaviour. 


## Group lasso

The group lasso allows to gather some of the covariates and treat those with the same coefficient jointly in the $L_1$-regularised problem. For the implementation in R we utilize the `grplasso` library. Naturally, we group the different levels of the factorial variables, i.e. the years of construction `bj` and the different levels for the districts `bez` together, respectively. 

```{r groupLasso, eval=TRUE, include=FALSE}
# Load library for group lasso
library(grplasso)

# Preparation of data frame
# Adding an intercept to the design matrix, because of new package now used
x_mod_group <- cbind(1, x_mod)
colnames(x_mod_group)[1] <- c("Intercept")
df_mod_group <- data.frame(y_mod,x_mod_group)
colnames(df_mod_group)[1] <-c("nm")

# Model definition
district <- rep(4,24) #25 levels of factor bez (1 level goes into intercept) 
year <- rep(3,43) #44 levels of factor bj (1 level goes itno intecept)
index_mod <- c(NA,1,2,year,district,5,6,7,8,9,10,11) #individual groups for all except bez and bj

grp_lambda_mod <- lambdamax(x=x_mod_group, y = y_mod, index = index_mod, penscale = sqrt,
                          model = LinReg(),standardize = TRUE,center = TRUE)*0.5^(0:40)


# fit using grouped index
grp_mod <- grplasso(x=x_mod_group, y = y_mod, index = index_mod, lambda = grp_lambda_mod[-(1:3)], model = LinReg(),
                             penscale = sqrt,
                             control = grpl.control(update.hess = "lambda", trace = 0))

## Plot coefficient paths
plot(grp_mod)
str(grp_mod)
coef(grp_mod)

```

Again we employ the same model selection criterion via cross validation as above, but implement it in R using the `gglasso` library. In the cross validation procedure, we can observe sudden jumps when a new group is taken into or out from the model, since the calculatations either include all or non of the levels of the factorial variables, what can yield to a jump in the number of parameters from 9 to 53 in a single step on the lmabda grid. 

```{r groupLassoCV, eval=T, include=F}
# Load gglasso library for cross validation
library(gglasso)
# Use own defined lambdagrid
fitls_mod <- gglasso(x = x_mod_group[,-1], y = y_mod, group = index_mod[-1], loss = "ls",lambda=grp_lambda_mod)
plot(fitls_mod)

# crossvalidation on self defined lambdagrid 
# WARNING: this takes some time
cvfitls_mod <- cv.gglasso(x = x_mod_group[,-1], y = y_mod, group = index_mod[-1], loss = "ls",lambda=grp_lambda_mod)
plot(cvfitls_mod)
str(cvfitls_mod)
cvfitls_mod$lambda.1se

#fit with optimal lambda on defined lambda grid(either package made or made by me)
fin_mod=gglasso(x = x_mod_group[,-1], y = y_mod, group = index_mod[-1], loss = "ls",lambda=exp(cvfitls_mod$lambda.1se))
coef(fin_mod)
```

```{r groupLassoPlot, echo=FALSE, fig.cap="\\label{fig:group}Model selection for group lasso"}
# Plot for report
plot(fitls_mod)
abline(v=(cvfitls_mod$lambda.1se))
```

In Figure \ref{fig:group} we again depict the coefficients of the covariates against the $\log\lambda$ and highlight the optimal hyperparameter. By construction, all levels of a factorial variable are shrinked to simultaneously. This means that either all levels remain in the model or all levels are excluded, what corresponds better to the practical interpretation of those variables. 


## Conclusion

```{r final, eval=TRUE, include=FALSE}
#final results, group lasso either shrinks all or nothing and even increases some estimated parameter coefficients
results_mod=cbind(coef(lm_mod), coef(ridge_fit,s=cv_ridge$lambda.1se), coef(gen_mod,s=cv_gen_mod$lambda.1se), coef(fin_mod))
colnames(results_mod)=c("vanilla LS", "ridge", "general lasso", "group lasso")
print(results_mod)
```

In this project, we have chosen a practical data set which contains data on rental prices in Munich (two of the group members are practically familiar with the difficult housing situation in Munich and it was appealing to analyse this statistically). For the data analysis, the factorial variables needed special attention. A plain vanilla and ridge regression were not capable to give explainable outcomes. Likewise, the result of the lasso was contra-intuitive in the unclear handling of the factorial varibles. Finally, the group lasso where a factorial varible can be arranged together leads to an interpretable shrinkage conclusion, where all variables except the factorial are selected for the optimal hyperparemeter choise.



# Inference

The data set is too small to divide it into a training and a test set. 

Bootstrap can be applied here to find the proportion of times each element in the coefficients vector of being zero. So it is a way of validation. 

```{r,eval=T, tidy = F, include = F, fig.width=15, fig.height=5}
# boostrap loop
set.seed(2021)
B=100
n=nrow(x_mod)
p=ncol(x_mod)
lassomat=matrix(ncol=p+1,nrow=B)
ridgemat=matrix(ncol=p+1,nrow=B)

# no need or separate function for steps 1-6 since can use cv.glmnet
# and weight argument for giving the new bootstrapped data
for (b in 1:B)
{
  ids=sort(sample(1:n,replace=TRUE))
  wids=rep(0,n)
  for (i in 1:n)
    wids[i]=sum(ids==i)
  resl=cv.glmnet(x_mod,y_mod,weights=wids)
  resr=cv.glmnet(x_mod,y_mod,weights=wids,alpha=0)
  lassomat[b,]=as.vector(coef(resl)) #automatic lambda 1sd
  ridgemat[b,]=as.vector(coef(resr)) #automatic lambda 1sd
}
colnames(lassomat)=colnames(ridgemat)=c("Int.cept",colnames(x_mod))
# plotting boxplots
lassomatUI=lassomat[,-1]
lassods=reshape2::melt(lassomatUI,
         variable.name ="variable",value.name="value")
lassopp=ggplot(lassods,aes(x=Var2,y=value))+
  geom_boxplot()+ggtitle("Boxplots for boostrapped lasso for diabetes data")
# lassopp

ridgematUI=ridgemat[,-1]
ridgeds=reshape2::melt(ridgematUI,variable.name="variable",value.name="value")
ridgepp=ggplot(ridgeds,aes(x=Var2,y=value))+
  geom_boxplot()+ggtitle("Boxplots for boostrapped ridge for diabetes data")
# ridgepp

lasso0perc=apply(abs(lassomat)<.Machine$double.eps,2,mean)
par(mfrow = c(1, 1))
barplot(lasso0perc)
```





