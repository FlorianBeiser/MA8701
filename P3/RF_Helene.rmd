---
subtitle: "MA8701 Advanced Methods in Statistical Inference and Learning V2021"
title: "Data Analysis Project 2"
author: "Florian Beiser, Yaolin Ge  & Helene Minge Olsen"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
 # html_document
  pdf_document
---
  

```{r rpackages,eval=TRUE,echo=FALSE, message=FALSE, warning=FALSE}
library(lubridate)
library(tree)
library(randomForest)
library(data.table)
```

In this project, we analyse a real dataset using methods from part 2-4 of the MA8701 course.

### Note on Open Science

To pursue the idea of reproducible research, the chosen dataset as well as the code for our analysis are publicly accessible:

* dataset: https://github.com/metno/havvarsel-data-driven-pred
* code: https://github.com/FlorianBeiser/MA8701

# The Data Set 

For this project, we generated a data set ourselves by fetching real-world observations from the `Havvarsel-Forst` and `Frost` database servers of the Meteorological Institute of Norway (MET). To this end, we established a routine downloading from the respective API, which can be found in the aforementioned repository. Since the data was obviously not analyzed before in a similar way, the subsequent result will contribute to active research. 

The constructed data set consists of multiple time series with measurement of different weather properties and a time series for the water temperature at Sjøstrand in Asker kommune during the summer month, which was originally collected through badevann.no in the summer month from 2016 til 2020. Sjøstrand is a popular swimming site south of Oslo. As index, our data set contains the times of measurement which are typically made at 4 full hours per day. Technically, the water temperatures are collected in the respective column

- `water_temp` in `degC` measured at 7, 10, 14, 16 every day during swimming season.

The goal of the data analysis is the predict the swimming tempertures at Sjøstrand beach using atmospheric weather values (no forecasted values) from surrounding weather observation stations. In the aforementioned data generation process via the `Frost` database of MET we retrieve values for 7 different atmospheric weather values where we identify the stations for those measurements respectively and include them in our dataset. Thereby, we follow the naming convention: "station_id" (identifyer of the station where the measurement is done) + "element" (atmoshperic quantity that got measured) + "number" (0=measurement in 2m height, 1=measurement in 10 height above ground). As elements we consider: 

- `air_temperature` in $degC$
- `wind_speed` in $m/s$
- `cloud_area_fraction` in $0,...,8$ (0 no clouds, 8 fully cloudy)
- `mean(solar irradiance)` in $W/m^2$ - a quantity for the intensity of the sun
- `sum(duration of sunshine)` in $min$ - sunshine in the last hour
- `mean(realtive humidity)` in $%$
- `mean(downwelling shortwave flux)` in $W/m^2$ - a quantity for the intensity of the sun (similar to irradiance but only for highly intesive UV radiation)

The data generation process can be repeated (or modified) by running the `run_example.sh` in the data generation repository - however, the dataset which we subsequently use is also provided stationary in the code repository.

Therewith, we end up a data set with a time index, one `water_temp` measurement, and 14 atmospheric measurements, where we want to do prediction of the water temperature (what will be explained in a bit more detail in the subsequent sections).


## Model Building

Our goal is to predict the water temperature at the next time using all information from the previous time - this includes the `water_temp` at that previous time. Therefore, we introduce all atmospheric observations \emph{and} the water temperature at time $t$ as covariates in order to model the response, which is the water temperature at time $t+1$. (Note that we explicitly assume here that we do not use forecasted values for the atmospheric variables even if that would be a valid data source in further research projects.)

In a nutshell, the dataset consists of 15 covariates and 2327 observations. 


## Preprocessing 

Even if atmospheric measurements may exists for more times than the water temperature, we restrict ourselves to those time points when water tempeature measurements exists. However, it happens that the measurement equipment is out of order for a short period of time (it happens but it happens only very seldomly and then typically one day of atmospheric observations at a station are missing). This data is missing completely at random and within the data generation process it is imputed from its nearest neighbor. Even though this method is commonly not recommended we argue that the weather does not completely change within a few hours and this guess is better than mean imputation (maybe in Norway the weather changes quicker than in other parts of the world, but only very very few atmospheric measurements are missing such that we do not expect any influence on the data analysis.)

The data covers 5 years such that we use the years 2016-2019 (about 80% of the data) as training set in the data analysis and we set the last year (2020, corresponding to about 20% of the data) apart as test data to validate the results.


```{r,eval=T,echo=F}
#############################
## Data preprocessing

# Loading data
raw_data <- read.csv("dataset1.csv")

# Creating a response column by extracting, shifting and adding "water_temp"
response_column <- shift(raw_data[,"water_temp"],n=1)
data <- cbind(raw_data,response_column)
colnames(data)[19] <- "response"

#remove first row (no response)
data <- data[-1,]

#remove empty columns
data <- data[,!(names(data) %in% c("SN19940wind_speed1","SN18700cloud_area_fraction1"))]

#create train and test set
# @HMO: Why was data resampled??
train_indicies = c(1:length(data$time[year(data$time)<2020]))
#2016-2019 train
data_train <- data[train_indicies,]
#2020 test
data_test <- data[-train_indicies,]

print(head(data))

```

## Data Exploration

We start the exploration by investigating the response. 

```{r, eval=T, echo=F}
#visualize data
season = as.POSIXct(data$time[year(data$time)==2016],format="%Y-%m-%d %H:%M:%S")
year(season) = 0

plot(season, 
    data$water_temp[year(data$time)==2016],
    main="water temperatures", type="l",
    xlab="time", ylab="degC",
    xlim=c(as.POSIXct("0000-05-01 00:00:00",format="%Y-%m-%d %H:%M:%S"),
           as.POSIXct("0000-10-30 00:00:00",format="%Y-%m-%d %H:%M:%S")),
    ylim=c(5,25))

col_idx = 2
for (y in 2017:2020){
  season = as.POSIXct(data$time[year(data$time)==y],format="%Y-%m-%d %H:%M:%S")
  year(season) = 0
  lines(season, data$water_temp[year(data$time)==y], col=col_idx)
  col_idx = col_idx + 1 
}

legend("topright", legend=c(2016,2017,2018,2019,2020),
       col=c(1,2,3,4,5), lty=1)
```

The water temperature shows a very pronounced seasonality in its progression. Moreover, it changes only rather slow from one time to the next. 


```{r, eval=T, echo=F}
par(mfrow=c(1,2))

#visualize data
season = as.POSIXct(data$time[year(data$time)==2016],format="%Y-%m-%d %H:%M:%S")
year(season) = 0

plot(season, 
    data$"SN19710air_temperature0" [year(data$time)==2016],
    main="air temperatures (SN19710)", type="l",
    xlab="time", ylab="degC",
    xlim=c(as.POSIXct("0000-05-01 00:00:00",format="%Y-%m-%d %H:%M:%S"),
           as.POSIXct("0000-10-30 00:00:00",format="%Y-%m-%d %H:%M:%S")),
    ylim=c(0,35))

col_idx = 2
for (y in 2017:2020){
  season = as.POSIXct(data$time[year(data$time)==y],format="%Y-%m-%d %H:%M:%S")
  year(season) = 0
  lines(season, data$"SN19710air_temperature0" [year(data$time)==y], col=col_idx)
  col_idx = col_idx + 1 
}

legend("topright", legend=c(2016,2017,2018,2019,2020),
       col=c(1,2,3,4,5), lty=1)


#visualize data
season = as.POSIXct(data$time[year(data$time)==2016],format="%Y-%m-%d %H:%M:%S")
year(season) = 0

plot(season, 
    data$"SN50539mean.solar_irradiance.PT1H.0" [year(data$time)==2016],
    main="mean solar irradience (SN50539)", type="l",
    xlab="time", ylab="W/m2",
    xlim=c(as.POSIXct("0000-05-01 00:00:00",format="%Y-%m-%d %H:%M:%S"),
           as.POSIXct("0000-10-30 00:00:00",format="%Y-%m-%d %H:%M:%S")),
    ylim=c(0,1000))

col_idx = 2
for (y in 2017:2020){
  season = as.POSIXct(data$time[year(data$time)==y],format="%Y-%m-%d %H:%M:%S")
  year(season) = 0
  lines(season, data$"SN50539mean.solar_irradiance.PT1H.0" [year(data$time)==y], col=col_idx)
  col_idx = col_idx + 1 
}

legend("topright", legend=c(2016,2017,2018,2019,2020),
       col=c(1,2,3,4,5), lty=1)

```
In some of the covariates this seasonality is also present, but with an much higher volatility, and in some covariates we cannot recognize any significant seasonality at all and additionally they change much faster - as example we show the air temperature and the solar irradiance averaged over the last hour at their observation station which is closest to Sjøstrand. (The lonlat-coordinates of the relevant stations can be found in the `log1.txt` file in the code repository.)

Further the seaborn pairplot (see python part of this project) shows sometimes clear linear correlation between the covariates and the response, but also between the covariates themselves, which is "dangerous" as we have seen in Kjerstis lectures. However, also very unclear relations between the response and the covariates are visible.

This yields that non-linear data analysis methods are needed for the dataset at hand.


# Random Forest Model Fit 

Since the water temperature changes rather slowly compared to the atmospheric quantities and we have the water temperature of the previous time included as covariate, we know that this strong predictor `water_temp` will dominate the trees produced if we choose to use bagging. To obtain decorrelated trees and improve the variance reduction, we therefore want to fit a random forest model to our data set. 

We utilize the `randomForest` package in R. As the random forest allows a random selection of $m$ covariates $m \leq p$ to be considered for the split for each node, a 5-fold cross validation was applied to find the optimum number of covariates. As the data set contains 19 variables, the general rule for regression is to set the number of randomly chosen variables to be considered as $floor(\frac{19}{3}) = 6$. However, we obtain the smallest cross-validation error for $m=4$. This can be explained by our data set being very correlated, and it is therefore wise to set $m$ to be small. 

```{r,eval=TRUE,echo=FALSE}
library(randomForest)
#CV to find optimal mtry
#WARNING! takes a long time to run 
randomForest::rfcv(trainx = data_train[,!(names(data_train)=="response")],trainy = data_train[,"response"])$n.var

#build a random forest with mtry = 4 
rf_temp <- randomForest(response ~ .-time, data = data, subset = train_indicies, mtry =4, importance= TRUE)
#to see fit summary uncomment next line
rf_temp

```

```{r,eval=TRUE,echo=FALSE,tidy = F, fig.width=10, fig.height=5, fig.cap="\\label{fig:RFRunPlot} Plot of numbers of trees"}
plot(rf_temp, main = "")
```

As each tree uses a different bootstrap sample, the OOB sample is used as a validation set. From Figure \ref{fig:RFRunPlot} we observe how the OOB error estimate is reduced as a function of the nr of trees. As a result, the model fits 500 trees to average over, with a percentage of variance explained, also known as pseudo R-squared, around 91.97$\%$

```{r,eval=TRUE,echo=FALSE,tidy = F, fig.width=10, fig.height=5, fig.cap="\\label{fig:RFTestPlot} Predicted test values against true values."}
#test
yhat_rf <- predict(rf_temp, newdata = data_test)
temp_plot <- data_test[,"response"] 
plot(yhat_rf, temp_plot, xlab = "Predicted Temperature", ylab = "True Temperature")
abline(0,1)
```

Figure \ref{fig:RFTestPlot} depicts a good fit except for a slight overprediction for lower temperatures between 9 and 13 degrees. 

```{r,eval=TRUE,echo=FALSE, tidy = F, fig.width=10, fig.height=5, fig.cap="\\label{fig:ImportancePlotMSE} Variable importance plot based on increase in MSE when premuted and tested using OOB sample, higher values indicate larger impact when premuted, hence larger importance."}
#variable importance based on randomization - higher value mean more important
#incMSE OOB
varImpPlot(rf_temp, type = 1,main = "")
```

```{r,eval=TRUE,echo=FALSE, tidy = F, fig.width=10, fig.height=5, fig.cap="\\label{fig:ImportancePlotPure} Variable importance plot based on increase in node purity, higher values indicate larger importance."}
#incNodePurity
varImpPlot(rf_temp, type = 2,main = "")
```


Figure \ref{fig:ImportancePlotMSE} and \ref{fig:ImportancePlotPure} depicts that the variable importance plot based on the OOB sample tends to spread the importances more uniformly, but `water_temp` remains a main predictor for both. We observe that there are a lot of predictors considered to be relevant after `water_temp`, which indicates a well performance from the random forest model. This is reflected in the low test MSE.

```{r,eval=TRUE,echo=FALSE}
#MSE on test set for bagged tree
cat("Test MSE of Random Forest Model\n",mean((yhat_rf-temp_plot)^2))
```
